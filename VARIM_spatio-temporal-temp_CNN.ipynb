{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os \n",
    "import sys\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "import requests\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import PIL.Image as Image\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.keras.layers import Input\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import BatchNormalization, Flatten, Input, Dense,Dropout,Activation,Flatten,Reshape,Lambda,Embedding,Conv2D,MaxPooling2D, UpSampling2D   #卷积层，池化层\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import tensorflow.python.platform.build_info as build \n",
    "import tensorflow_hub as hub \n",
    "import tensorflow_datasets as tfds \n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "%cd C:\\Results_2m_0.56m_0.0683m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def getTemp(starting_frame, ending_frame, info_current_dataset, img_current_dataset):\n",
    "  for frame in range(starting_frame, ending_frame + 1):\n",
    "    temp_filename = \"TEMPERATURE_at_frame%d.txt\" % frame \n",
    "    time_stamp_raw = pd.read_csv(temp_filename, sep='\\t', header=None, \n",
    "                                skiprows=[i for i in range(0,4)], on_bad_lines='skip')\n",
    "    timestamp = [float(s) for s in re.findall(r':\\s(.*)', time_stamp_raw.iat[2, 1])]\n",
    "    timestamp = timestamp[0]    \n",
    "      \n",
    "    temp_raw = pd.read_csv(temp_filename, header=None, sep='\\t', \n",
    "                          skiprows=[i for i in range(0,9)]).iloc[[i for i in range(0, 17583 - 9)], [0, 1]]\n",
    "    temp_raw.columns = ['node_label', 'temperature']\n",
    "    temp = pd.merge(temp_raw, node_info, on = 'node_label')\n",
    "    temp['step'] = frame\n",
    "    temp['temperature'] = temp['temperature'] - 273.15\n",
    "\n",
    "    if (frame == starting_frame):\n",
    "      initial_time = timestamp\n",
    "      x_dim = (temp['node_x'].unique()).size\n",
    "      y_dim = (temp['node_y'].unique()).size\n",
    "      z_dim = (temp['node_z'].unique()).size\n",
    "\n",
    "    temp['time'] = (timestamp - initial_time)/60   \n",
    "    temp['scaled_x'] = ( temp['node_x'] / max(temp['node_x']) ) * x_dim\n",
    "    temp['scaled_y'] = ( (temp['node_y']) / (max(temp['node_y'])) ) * y_dim\n",
    "    temp['scaled_z'] = ( (temp['node_z']) / (max(temp['node_z'])) ) * z_dim\n",
    "\n",
    "    # cast original coordinates values into integers\n",
    "    temp['X'] = temp.scaled_x.astype('int')\n",
    "    temp['Y'] = temp.scaled_y.astype('int')\n",
    "    temp['Z'] = temp.scaled_z.astype('int')\n",
    "    info_current_dataset.append(temp)                     # append current frame\n",
    "    \n",
    "    # cast temperature data into images\n",
    "    img_x = max(temp['X']) + 1   # 102 \n",
    "    img_y = max(temp['Y']) + 1   # 30 \n",
    "    img_z = max(temp['Z']) + 1   # 6  \n",
    "\n",
    "    get_image = np.zeros(shape = (img_y, img_x, img_z))\n",
    "\n",
    "    for i in range(len(temp)):\n",
    "      x_coordinate = temp.loc[i, \"X\"]\n",
    "      y_coordinate = temp.loc[i, \"Y\"]\n",
    "      z_coordinate = temp.loc[i, \"Z\"]\n",
    "      get_image[y_coordinate][x_coordinate][z_coordinate] = temp.loc[i, \"temperature\"]\n",
    "    img_current_dataset.append(get_image)                 # append current frame\n",
    " \n",
    "def getImgChannel(channel, dataset):\n",
    "  output_img = []\n",
    "  length = len(dataset)\n",
    "  for i in range(length):\n",
    "    output_img.append(dataset[i, :, :, channel])\n",
    "  return np.array(output_img)\n",
    "\n",
    "def get_all_ImgChannel(dataset):\n",
    "  output_img = []\n",
    "  length = len(dataset)\n",
    "  for i in range(length):\n",
    "    output_img.append(dataset[i, :, :, 0])\n",
    "  for i in range(length):\n",
    "    output_img.append(dataset[i, :, :, 1])\n",
    "  for i in range(length):\n",
    "    output_img.append(dataset[i, :, :, 2])\n",
    "  for i in range(length):\n",
    "    output_img.append(dataset[i, :, :, 3])\n",
    "  for i in range(length):\n",
    "    output_img.append(dataset[i, :, :, 4])\n",
    "  for i in range(length):\n",
    "    output_img.append(dataset[i, :, :, 6])\n",
    "  return np.array(output_img)\n",
    "\n",
    "def patch_image(dataset):\n",
    "  for img in range(len(dataset)):\n",
    "    for col in range(102):\n",
    "      (dataset[img])[28, col] = ((dataset[img])[27, col] + (dataset[img])[29, col]) / 2\n",
    "    for row in range(30):\n",
    "      (dataset[img])[row, 100] = ((dataset[img])[row, 99] + (dataset[img])[row, 101]) / 2\n",
    "    (dataset[img])[28, 100] = ((dataset[img])[27, 99] + (dataset[img])[27, 101] + (dataset[img])[29, 99] + (dataset[img])[29, 101]) / 4\n",
    "\n",
    "################################################################################\n",
    "node_info = pd.read_csv('Mesh_info.txt', delim_whitespace=True, header=None)\n",
    "node_info.columns = ['node_label', 'node_x', 'node_y', 'node_z']\n",
    "node_info = node_info.astype(str).astype(float)\n",
    "\n",
    "temperature_info_all = [] \n",
    "temperature_img_all = []\n",
    "\n",
    "getTemp(1, 541, temperature_info_all, temperature_img_all)\n",
    "\n",
    "temperature_img_all_array = np.array(temperature_img_all)\n",
    "\n",
    "layer_z6 = getImgChannel(6, temperature_img_all_array)\n",
    "patch_image(layer_z6)\n",
    "layer_z6_norm = layer_z6 / np.max(layer_z6)\n",
    "\n",
    "images_all_layers = get_all_ImgChannel(temperature_img_all_array)\n",
    "patch_image(images_all_layers)\n",
    "images_all_layers_norm = images_all_layers / np.max(images_all_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### CNN architecture ##########################\n",
    " \n",
    "img_size = Input(shape = (30, 102, 1), name = 'Input') \n",
    "\n",
    "x = Conv2D(16, (3, 3), strides = (1, 4),activation = 'relu', padding = 'same', name = 'enc_1_conv')(img_size)\n",
    "x = MaxPooling2D((2, 2), padding = 'same', name = 'enc_2')(x)\n",
    "x = Conv2D(32, (3, 3), activation = 'relu', padding = 'same', name = 'enc_3_conv')(x)\n",
    "x = MaxPooling2D((2, 2), padding = 'same', name = 'enc_4')(x)\n",
    "x = Conv2D(64, (3, 3), activation = 'relu', name = 'enc_5_conv')(x)\n",
    "x = MaxPooling2D((2, 2), padding = 'same', name = 'enc_6')(x)\n",
    "x = Conv2D(128, (3, 3), activation = 'relu', name = 'enc_7_conv')(x)\n",
    "encoded = Flatten(name = 'enc_8')(x)\n",
    "\n",
    "########################  Decoder ########################\n",
    "\n",
    "x = Reshape((1, 1, 128), name = 'dec_1')(encoded)\n",
    "x = UpSampling2D((2, 2), name = 'dec_2')(x)\n",
    "x = Conv2D(64, (3, 3), activation = 'relu', padding = 'same', name = 'dec_3_conv')(x)\n",
    "x = UpSampling2D((2, 2), name = 'dec_4')(x)\n",
    "x = Conv2D(32, (3, 3), activation = 'relu', padding = 'same', name = 'dec_5_conv')(x)\n",
    "x = UpSampling2D((3, 3), name = 'dec_6')(x)\n",
    "x = Conv2D(16, (3, 3), activation = 'relu', name = 'dec_7_conv')(x)\n",
    "x = UpSampling2D((3, 3), name = 'dec_8')(x)\n",
    "x = Conv2D(16, (1, 3), activation = 'relu', name = 'dec_9_conv')(x)\n",
    "x = Conv2D(16, (1, 3), activation = 'relu', name = 'dec_10_conv')(x)\n",
    "x = UpSampling2D((1, 4), name = 'dec_11')(x)\n",
    "decoded = Conv2D(1, (1, 3), activation = 'relu', name = 'dec_12_conv')(x)\n",
    "\n",
    "autoencoder = Model(img_size, decoded)\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "train_all, test_dataset = train_test_split(images_all_layers_norm, test_size = 0.2, random_state = 42)\n",
    "train, validation = train_test_split(train_all, test_size = 0.2, random_state = 42)\n",
    "\n",
    "train = train.reshape(train.shape[0], 30, 102, 1)\n",
    "validation = validation.reshape(validation.shape[0], 30, 102, 1)\n",
    "test = test_dataset.reshape(test_dataset.shape[0], 30, 102, 1)\n",
    "\n",
    "epoch = 1500;\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=100, verbose=2),\n",
    "             ModelCheckpoint('CNN-test.h5', monitor = 'val_loss', save_best_only = True, verbose=0)]\n",
    "\n",
    "keras.backend.set_epsilon(1)\n",
    "autoencoder.compile(loss='mae', optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001), \n",
    "                    metrics=['mape'])   \n",
    "\n",
    "history = autoencoder.fit(train, train, batch_size = 128, epochs = epoch, verbose = 1, \n",
    "                          validation_data = (validation, validation), workers=8, \n",
    "                          use_multiprocessing = 1, callbacks = callbacks) \n",
    "\n",
    "plt.plot(history.history['mape'], label = 'Training data')\n",
    "plt.plot(history.history['val_mape'], label ='Validation data')\n",
    "plt.ylabel('MAPE')\n",
    "plt.xlabel('No. epoch')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_testing = autoencoder.evaluate(test, test, verbose = 2)\n",
    "model_testing"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
